import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm
import h5py
import numpy as np
import sys
import os

# å¼•å…¥ä½ çš„ç½‘ç»œå®šä¹‰
sys.path.append('DataLoader')
# å‡è®¾ PhysicsGuidedNetwork.py å’Œ Train è„šæœ¬åœ¨åŒä¸€çº§æˆ–èƒ½è¢« python path æ‰¾åˆ°
try:
    from PhysicsGuidedNetwork import PhysicsGuidedNet
except ImportError:
    # å°è¯•ç›´æ¥ä»å½“å‰ç›®å½•å¯¼å…¥
    from PhysicsGuidedNetwork import PhysicsGuidedNet

# ================= é…ç½®åŒºåŸŸ =================
H5_PATH = r"D:\Dataset\SignalDataset\merged_dataset_512_3d_fast.h5"
BATCH_SIZE = 32
CHUNK_SIZE = 4000  # ã€å…³é”®ã€‘æ¯æ¬¡è¯»å…¥å†…å­˜çš„æ ·æœ¬æ•°ã€‚2000ä¸ªæ ·æœ¬çº¦å  4GB å†…å­˜ã€‚æ ¹æ®ä½ çš„å†…å­˜å¤§å°è°ƒæ•´ã€‚
LR = 1e-4
EPOCHS = 50
SCENE_SIZE = 5000.0
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


def validate(model, val_indices, h5_file, criterion_coord, criterion_mask, chunk_size=1000, batch_size=32):
    """
    ä¿®å¤åçš„éªŒè¯å‡½æ•°ï¼šåŠ è½½å¤§å—æ•°æ®åï¼Œä½¿ç”¨ DataLoader åˆ†å°æ‰¹æ¬¡éªŒè¯ï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
    """
    model.eval()
    total_loss = 0.0
    total_dist_err = 0.0
    num_samples = 0

    # éªŒè¯é›†åˆ†å—åŠ è½½ (HDD -> RAM)
    with torch.no_grad():
        for i in range(0, len(val_indices), chunk_size):
            # 1. è·å–å½“å‰å—çš„ç´¢å¼•
            current_indices = val_indices[i: i + chunk_size]
            current_indices = np.sort(current_indices)  # HDF5 è¦æ±‚å‡åº

            # 2. åŠ è½½åˆ° CPU å†…å­˜ (RAM)
            # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œç›´æ¥ .to(DEVICE)ï¼Œå¦åˆ™ 1000 æ¡æ•°æ®ä¼šå æ»¡æ˜¾å­˜
            iq_ram = torch.from_numpy(h5_file['iq'][current_indices]).float()
            heatmap_ram = torch.from_numpy(h5_file['heatmap'][current_indices]).float()
            mask_ram = torch.from_numpy(h5_file['mask'][current_indices]).float()
            coord_ram = torch.from_numpy(h5_file['coord'][current_indices]).float()

            # 3. åˆ›å»ºä¸´æ—¶ DataLoader (RAM -> GPU Mini-batch)
            # è¿™æ ·æ¯æ¬¡åªå–‚ 32 æ¡ç»™ GPU
            temp_dataset = TensorDataset(iq_ram, heatmap_ram, coord_ram, mask_ram)
            temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

            # 4. å°æ‰¹æ¬¡æ¨ç†
            for iq, heatmap, true_coord, mask in temp_loader:
                iq, heatmap = iq.to(DEVICE), heatmap.to(DEVICE)
                mask, true_coord = mask.to(DEVICE), true_coord.to(DEVICE)

                # é¢„æµ‹
                pred_coord, pred_mask = model(iq, heatmap)

                # Loss
                true_coord_xy = true_coord[:, :2]
                loss_c = criterion_coord(pred_coord, true_coord_xy)
                loss_m = criterion_mask(pred_mask, mask)

                # ç´¯åŠ è¯¯å·® (ä¹˜ä»¥å½“å‰ batch å¤§å°)
                batch_len = iq.size(0)
                total_loss += (loss_c + 0.5 * loss_m).item() * batch_len

                dist_meter = torch.norm(pred_coord - true_coord_xy, dim=1) * SCENE_SIZE
                total_dist_err += dist_meter.sum().item()

                num_samples += batch_len

            # é‡Šæ”¾ RAM
            del iq_ram, heatmap_ram, mask_ram, coord_ram, temp_dataset, temp_loader

    return total_loss / num_samples, total_dist_err / num_samples


def main():
    print(f"ğŸš€ å¯åŠ¨åˆ†å—è®­ç»ƒ | Chunk Size: {CHUNK_SIZE}")

    if not os.path.exists(H5_PATH):
        print("æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ï¼")
        return

    # 1. æ‰“å¼€ HDF5 (åªè¯»å–å…ƒæ•°æ®ï¼Œä¸è¯»å†…å®¹)
    f = h5py.File(H5_PATH, 'r')
    total_samples = len(f['iq'])
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")

    # 2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (ç´¢å¼•åˆ’åˆ†)
    all_indices = np.arange(total_samples)
    # ä¸æ‰“ä¹±æ€»ç´¢å¼•ï¼Œç›´æ¥æŒ‰å‰90%å10%åˆ‡åˆ†ï¼Œä¿è¯è®­ç»ƒé›†åœ¨ç¡¬ç›˜ä¸Šæ˜¯è¿ç»­çš„ï¼Œè¯»å–æœ€å¿«
    split_idx = int(0.9 * total_samples)
    train_indices_all = all_indices[:split_idx]
    val_indices_all = all_indices[split_idx:]

    print(f"è®­ç»ƒé›†: {len(train_indices_all)}, éªŒè¯é›†: {len(val_indices_all)}")

    # 3. åˆå§‹åŒ–æ¨¡å‹
    sample_iq = f['iq'][0]
    num_rx = sample_iq.shape[0] // 2
    model = PhysicsGuidedNet(num_rx=num_rx, signal_len=2048).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    criterion_coord = nn.MSELoss()
    criterion_mask = nn.MSELoss()

    best_err = float('inf')

    # ================= è®­ç»ƒå¾ªç¯ =================
    for epoch in range(EPOCHS):
        model.train()
        train_loss_epoch = 0.0
        processed_samples = 0

        # è¿›åº¦æ¡
        pbar = tqdm(total=len(train_indices_all), desc=f"Epoch {epoch + 1}/{EPOCHS}")

        # --- åˆ†å—åŠ è½½å¾ªç¯ (Chunk Loading) ---
        # æ¯æ¬¡åªå¤„ç† train_indices_all ä¸­çš„ä¸€éƒ¨åˆ†
        # ä¸ºäº†ä¿è¯ I/O æœ€å¿«ï¼Œæˆ‘ä»¬æŒ‰é¡ºåºåˆ‡ç‰‡è¯»å–

        for chunk_start in range(0, len(train_indices_all), CHUNK_SIZE):
            chunk_end = min(chunk_start + CHUNK_SIZE, len(train_indices_all))

            # A. ã€åŠ è½½é˜¶æ®µã€‘ä»ç¡¬ç›˜è¯»å…¥å†…å­˜
            # ä½¿ç”¨åˆ‡ç‰‡ f['key'][start:end] æ˜¯æœ€å¿«çš„é¡ºåºè¯»å–æ–¹å¼
            # æ³¨æ„ï¼šè¿™é‡Œçš„ç´¢å¼•æ˜¯ç›¸å¯¹äº HDF5 æ–‡ä»¶çš„ç»å¯¹ç´¢å¼•
            # å› ä¸ºæˆ‘ä»¬åœ¨ä¸Šé¢æ˜¯æŒ‰é¡ºåºåˆ’åˆ†çš„ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥åˆ‡ç‰‡

            # print(f"  Loading chunk {chunk_start}-{chunk_end} to RAM...")
            iq_ram = torch.from_numpy(f['iq'][chunk_start:chunk_end])
            map_ram = torch.from_numpy(f['heatmap'][chunk_start:chunk_end])
            mask_ram = torch.from_numpy(f['mask'][chunk_start:chunk_end])
            coord_ram = torch.from_numpy(f['coord'][chunk_start:chunk_end])

            # B. ã€æ„é€ å†…å­˜ DataLoaderã€‘
            # æ•°æ®å·²ç»åœ¨å†…å­˜é‡Œäº†ï¼ŒTensorDataset åŒ…è£…ä¸€ä¸‹
            # num_workers=0, å› ä¸ºå†…å­˜è¯»å–ä¸éœ€è¦å¤šè¿›ç¨‹ï¼Œå¤šè¿›ç¨‹åè€Œæ…¢
            mem_dataset = TensorDataset(iq_ram, map_ram, coord_ram, mask_ram)
            train_loader = DataLoader(mem_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

            # C. ã€GPU è®­ç»ƒé˜¶æ®µã€‘
            for iq, heatmap, true_coord, mask in train_loader:
                iq, heatmap, mask, true_coord = iq.to(DEVICE), heatmap.to(DEVICE), mask.to(DEVICE), true_coord.to(
                    DEVICE)

                optimizer.zero_grad()
                pred_coord, pred_mask = model(iq, heatmap)

                true_coord_xy = true_coord[:, :2]
                loss_c = criterion_coord(pred_coord, true_coord_xy)
                loss_m = criterion_mask(pred_mask, mask)
                loss = loss_c + 0.5 * loss_m

                loss.backward()
                optimizer.step()

                train_loss_epoch += loss.item() * iq.size(0)
                processed_samples += iq.size(0)

            # æ›´æ–°æ€»è¿›åº¦æ¡
            pbar.update(chunk_end - chunk_start)
            pbar.set_postfix({'Loss': f"{loss.item():.4f}"})

            # D. ã€é‡Šæ”¾å†…å­˜ã€‘
            # è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯å‰ï¼Œiq_ram ç­‰å˜é‡ä¼šè¢«è¦†ç›–æˆ–é”€æ¯ï¼ŒPython GC ä¼šè‡ªåŠ¨å›æ”¶
            del iq_ram, map_ram, mask_ram, coord_ram, mem_dataset, train_loader

        pbar.close()

        # --- éªŒè¯é˜¶æ®µ ---
        print("Validating...")
        # ä¿®æ”¹è°ƒç”¨æ–¹å¼ï¼Œä¼ å…¥ batch_size
        avg_val_loss, avg_dist_err = validate(
            model,
            val_indices_all,
            f,
            criterion_coord,
            criterion_mask,
            chunk_size=CHUNK_SIZE,  # ä½¿ç”¨å’Œè®­ç»ƒä¸€æ ·çš„ Chunk å¤§å°è¯»å–ç¡¬ç›˜
            batch_size=BATCH_SIZE  # ä½¿ç”¨å’Œè®­ç»ƒä¸€æ ·çš„ Batch å¤§å°è¿›è¡Œæ¨ç†
        )

        avg_train_loss = train_loss_epoch / len(train_indices_all)
        print(
            f"Epoch {epoch + 1} Result: Train Loss={avg_train_loss:.5f}, Val Loss={avg_val_loss:.5f}, Err={avg_dist_err:.2f}m")

        if avg_dist_err < best_err:
            best_err = avg_dist_err
            torch.save(model.state_dict(), "best_model_chunked.pth")
            print(">>> Model Saved!")

    f.close()


if __name__ == '__main__':
    main()