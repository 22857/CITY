import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import numpy as np
# å¯¼å…¥ç½‘ç»œå’Œæ•°æ®é›†
from PhysicsGuidedNetwork import PhysicsGuidedNet
from PhysicsGuidedDataset import PhysicsGuidedHDF5Dataset

# ================= é…ç½®åŒºåŸŸ =================

# 1. æ•°æ®é›†è·¯å¾„ (æŒ‡å‘ MakeCsvIQData -> Generate_Multimodal_Data ç”Ÿæˆçš„ç‹¬ç«‹æ–‡ä»¶)
TRAIN_H5_PATH = "/root/autodl-tmp/merged_dataset_512_3d_train.h5"
VAL_H5_PATH = "/root/autodl-tmp/merged_dataset_512_3d_valid.h5"

# 2. ä¿å­˜è·¯å¾„
SAVE_PATH = "best_model_urban_512.pth"
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 3. æ ¸å¿ƒè®­ç»ƒå‚æ•°
# 24G æ˜¾å­˜ (3090/4090) -> 32
# 16G æ˜¾å­˜ (V100/T4)   -> 16
# 12G æ˜¾å­˜ (1080Ti)    -> 8
BATCH_SIZE = 32
NUM_WORKERS = 8
LR = 1e-4
EPOCHS = 50
SCENE_SIZE = 5000.0

# ã€å…³é”®ã€‘æ˜ç¡®æŒ‡å®šåˆ†è¾¨ç‡å’Œæ¥æ”¶æœºæ•°é‡ï¼Œå¿…é¡»ä¸æ•°æ®ç”Ÿæˆä¸€è‡´
MAP_SIZE = 512
NUM_RX = 6
SIGNAL_LEN = 2048


# ================= å·¥å…·æ¨¡å— =================

def apply_augmentation(iq, heatmap, coord, mask):
    """
    æ•°æ®å¢å¼ºï¼šéšæœºç¿»è½¬ Heatmap å’Œ Maskï¼Œå¹¶åŒæ­¥è°ƒæ•´åæ ‡
    æ³¨æ„ï¼šæš‚ä¸ç¿»è½¬ IQ é€šé“ï¼Œé¿å…å¤æ‚çš„ 6Rx ç´¢å¼•æ˜ å°„é—®é¢˜
    """
    # éšæœºæ°´å¹³ç¿»è½¬ (H-Flip)
    if np.random.rand() > 0.5:
        heatmap = torch.flip(heatmap, [3])
        mask = torch.flip(mask, [3])
        coord[:, 0] = 1.0 - coord[:, 0]

    # éšæœºå‚ç›´ç¿»è½¬ (V-Flip)
    if np.random.rand() > 0.5:
        heatmap = torch.flip(heatmap, [2])
        mask = torch.flip(mask, [2])
        coord[:, 1] = 1.0 - coord[:, 1]

    return iq, heatmap, coord, mask


class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred_logits, target):
        pred_probs = torch.sigmoid(pred_logits)
        # Flatten for Dice calculation
        pred_flat = pred_probs.view(pred_probs.size(0), -1)
        target_flat = target.view(target.size(0), -1)

        intersection = (pred_flat * target_flat).sum(1)
        dice = (2. * intersection + self.smooth) / (pred_flat.sum(1) + target_flat.sum(1) + self.smooth)
        return 1 - dice.mean()


def validate(model, loader):
    """
    éªŒè¯å‡½æ•°ï¼šè®¡ç®—å¹³å‡è·ç¦»è¯¯å·® (ç±³)
    å·²ç§»é™¤æ‰€æœ‰è¾¹ç¼˜è¿‡æ»¤é€»è¾‘ï¼Œå…¨é‡è¯„ä¼°
    """
    model.eval()
    total_dist_err = 0.0
    num_samples = 0

    with torch.no_grad():
        for iq, heatmap, coord, mask in loader:
            iq, heatmap, coord = iq.to(DEVICE), heatmap.to(DEVICE), coord.to(DEVICE)

            with torch.cuda.amp.autocast():
                pred_coord, _ = model(iq, heatmap)

            # è®¡ç®—çœŸå®è·ç¦»è¯¯å·® (Euclidean Distance)
            # coord[:, :2] æ˜¯å½’ä¸€åŒ–åæ ‡ (0~1)ï¼Œéœ€ä¹˜ SCENE_SIZE è¿˜åŸä¸ºç±³
            dist_err = torch.norm(pred_coord - coord[:, :2], dim=1) * SCENE_SIZE

            total_dist_err += dist_err.sum().item()
            num_samples += iq.size(0)

    if num_samples == 0: return 9999.0
    return total_dist_err / num_samples


# ================= ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨åŸå¸‚é«˜ç²¾å®šä½è®­ç»ƒ | {MAP_SIZE}x{MAP_SIZE} | {NUM_RX}Rx | è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“¦ Batch Size: {BATCH_SIZE}")

    # 1. åŠ è½½åŒæ•°æ®é›†
    print(f"Loading Train Set: {TRAIN_H5_PATH}")
    if not os.path.exists(TRAIN_H5_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶: {TRAIN_H5_PATH}")
    train_ds = PhysicsGuidedHDF5Dataset(TRAIN_H5_PATH)

    print(f"Loading Val Set:   {VAL_H5_PATH}")
    if not os.path.exists(VAL_H5_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°éªŒè¯æ–‡ä»¶: {VAL_H5_PATH}")
    val_ds = PhysicsGuidedHDF5Dataset(VAL_H5_PATH)

    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_ds)} | éªŒè¯æ ·æœ¬: {len(val_ds)}")

    # DataLoader
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=4, pin_memory=True)

    # 2. æ¨¡å‹åˆå§‹åŒ–
    # æ˜¾å¼ä¼ å…¥ map_size=512 ä»¥åŒ¹é… PhysicsGuidedNetwork ä¸­çš„å…¨è¿æ¥å±‚å®šä¹‰
    model = PhysicsGuidedNet(num_rx=NUM_RX, signal_len=SIGNAL_LEN, map_size=MAP_SIZE).to(DEVICE)

    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)
    scaler = torch.cuda.amp.GradScaler()

    # 3. Loss å®šä¹‰
    criterion_coord = nn.L1Loss()  # é»˜è®¤ mean reduction
    # é’ˆå¯¹ 512x512 çš„ç¨€ç–ç›®æ ‡ï¼Œç»™äºˆæ­£æ ·æœ¬æé«˜æƒé‡ (50.0)
    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([50.0]).to(DEVICE))
    criterion_dice = DiceLoss()

    best_err = float('inf')

    # 4. è®­ç»ƒå¾ªç¯
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}")

        for iq, heatmap, coord, mask in pbar:
            iq, heatmap, coord, mask = iq.to(DEVICE), heatmap.to(DEVICE), coord.to(DEVICE), mask.to(DEVICE)

            # æ•°æ®å¢å¼º
            iq, heatmap, coord, mask = apply_augmentation(iq, heatmap, coord, mask)

            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                # Forward
                pred_coord, pred_mask = model(iq, heatmap)

                # --- Loss Calculation ---

                # A. åæ ‡å›å½’ Loss (æ ¸å¿ƒä»»åŠ¡ï¼Œæƒé‡åŠ å€)
                loss_c = criterion_coord(pred_coord, coord[:, :2])

                # B. Mask åˆ†å‰² Loss (è¾…åŠ©ä»»åŠ¡ï¼Œæƒé‡é™ä½)
                loss_bce = criterion_bce(pred_mask, mask)
                loss_dice = criterion_dice(pred_mask, mask)
                loss_m = loss_bce + loss_dice

                # C. ä¸€è‡´æ€§ Loss (ç‹è€…å½’æ¥ï¼šå¸¦ IQ ç½®æ¢çš„ TTA)
                # åªæœ‰åŠ ä¸Šè¿™ä¸ª IQ ç½®æ¢ï¼ŒTTA æ‰æ˜¯å¯¹çš„ï¼
                loss_consistency = torch.tensor(0.0, device=DEVICE)
                if True:
                    # 1. ç¿»è½¬ Heatmap
                    heatmap_flip = torch.flip(heatmap, [3])

                    # 2. ã€å…³é”®ã€‘ç¿»è½¬ IQ é€šé“ (6Rx æ­£å…­è¾¹å½¢)
                    # ç´¢å¼•æ˜ å°„: Rx3, Rx2, Rx1, Rx0, Rx5, Rx4
                    idx_perm = torch.tensor([6, 7, 4, 5, 2, 3, 0, 1, 10, 11, 8, 9], device=DEVICE)
                    iq_flip = iq[:, idx_perm, :]

                    # 3. ä¼ å…¥ç¿»è½¬åçš„ iq_flip
                    pred_coord_flip, _ = model(iq_flip, heatmap_flip)

                    # 4. è¿˜åŸåæ ‡
                    pred_restored = pred_coord_flip.clone()
                    pred_restored[:, 0] = 1.0 - pred_restored[:, 0]

                    loss_consistency = torch.nn.functional.l1_loss(pred_coord, pred_restored.detach())

                # D. æ€» Loss (é‡æ–°é…æ¯”)
                # å¼ºåæ ‡(10.0)ï¼Œå¼±ç»˜å›¾(0.1)ï¼Œå¼ºä¸€è‡´æ€§(2.0)
                total_loss = 10.0 * loss_c + 0.1 * loss_m + 2.0 * loss_consistency

            # Backward
            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += total_loss.item()
            pbar.set_postfix({
                'Loss': f"{total_loss.item():.3f}",
                'L_c': f"{loss_c.item():.3f}",
                'Consis': f"{loss_consistency.item():.3f}"
            })

        # éªŒè¯é˜¶æ®µ
        val_err = validate(model, val_loader)
        print(f"Epoch {epoch + 1} éªŒè¯è¯¯å·®: {val_err:.2f}m")
        scheduler.step(val_err)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_err < best_err:
            best_err = val_err
            torch.save(model.state_dict(), SAVE_PATH)
            print(f"ğŸŒŸ æ–°çºªå½•: {best_err:.2f}m (å·²ä¿å­˜)")


if __name__ == '__main__':
    main()