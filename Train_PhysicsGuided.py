import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm
import h5py
import numpy as np
import os
from PhysicsGuidedNetwork import PhysicsGuidedNet

# ================= è·¯å¾„é…ç½® =================
# è¯·ç¡®ä¿è¯¥è·¯å¾„æŒ‡å‘ä½ ç”Ÿæˆçš„ 512x512 HDF5 æ–‡ä»¶
H5_PATH = r"D:\Dataset\SignalDataset\merged_dataset_512_3d_fast.h5"

# ================= è¶…å‚æ•°é…ç½® =================
BATCH_SIZE = 32  # GPU è®¡ç®—æ—¶çš„æ‰¹æ¬¡å¤§å°
CHUNK_SIZE = 2000  # æ¯æ¬¡ä»ç¡¬ç›˜è¯»å…¥å†…å­˜çš„æ ·æœ¬æ•°
LR = 1e-4  # åˆå§‹å­¦ä¹ ç‡
EPOCHS = 50  # æ€»è®­ç»ƒè½®æ•°
SCENE_SIZE = 5000.0  # åœºæ™¯ç‰©ç†å°ºå¯¸ (ç±³)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


# ================= éªŒè¯å‡½æ•° =================
def validate(model, val_indices, h5_file, criterion_coord, criterion_bce, criterion_dice, chunk_size=1000, batch_size=32):
    """
    åˆ†å—åŠ è½½éªŒè¯æ•°æ®ï¼Œå¹¶ä½¿ç”¨å° Batch æ¨ç†ï¼Œé˜²æ­¢ OOMã€‚
    """
    model.eval()
    total_loss = 0.0
    total_dist_err = 0.0
    num_samples = 0

    with torch.no_grad():
        # å¤–å±‚å¾ªç¯ï¼šåˆ†å—ä»ç¡¬ç›˜è¯»å…¥å†…å­˜
        for i in range(0, len(val_indices), chunk_size):
            # 1. è¯»å–å½“å‰å—æ•°æ®
            current_indices = val_indices[i: i + chunk_size]
            current_indices = np.sort(current_indices)  # HDF5 è¦æ±‚å‡åºç´¢å¼•

            # è¯»å…¥ CPU å†…å­˜ (RAM)
            iq_ram = torch.from_numpy(h5_file['iq'][current_indices]).float()
            heatmap_ram = torch.from_numpy(h5_file['heatmap'][current_indices]).float()
            mask_ram = torch.from_numpy(h5_file['mask'][current_indices]).float()
            coord_ram = torch.from_numpy(h5_file['coord'][current_indices]).float()

            # 2. æ„é€ ä¸´æ—¶ DataLoader (RAM -> GPU)
            temp_dataset = TensorDataset(iq_ram, heatmap_ram, coord_ram, mask_ram)
            temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

            # 3. å†…å±‚å¾ªç¯ï¼šå°æ‰¹æ¬¡æ¨ç†
            for iq, heatmap, true_coord, mask in temp_loader:
                iq, heatmap = iq.to(DEVICE), heatmap.to(DEVICE)
                mask, true_coord = mask.to(DEVICE), true_coord.to(DEVICE)

                # é¢„æµ‹
                pred_coord, pred_mask = model(iq, heatmap)

                # Loss è®¡ç®— (æ³¨æ„ 3D -> 2D åˆ‡ç‰‡)
                true_coord_xy = true_coord[:, :2]
                loss_c = criterion_coord(pred_coord, true_coord_xy)
                # ä½¿ç”¨ä¼ å…¥çš„æ··åˆ Loss è®¡ç®—éªŒè¯é›†æŸå¤±
                loss_b = criterion_bce(pred_mask, mask)
                loss_d = criterion_dice(pred_mask, mask)

                # éªŒè¯é›†æƒé‡å›ºå®šå³å¯ï¼Œä¸»è¦å‚è€ƒ dist_err
                loss_total = loss_c + 0.5 * (loss_b + loss_d)

                # ç´¯åŠ  Loss
                batch_len = iq.size(0)
                total_loss += loss_total.item() * batch_len  # ä½¿ç”¨è®¡ç®—å‡ºçš„ loss_total

                # ç´¯åŠ è·ç¦»è¯¯å·® (ç±³)
                dist_meter = torch.norm(pred_coord - true_coord_xy, dim=1) * SCENE_SIZE
                total_dist_err += dist_meter.sum().item()

                num_samples += batch_len

            # æ‰‹åŠ¨é‡Šæ”¾å†…å­˜
            del iq_ram, heatmap_ram, mask_ram, coord_ram, temp_dataset, temp_loader

    return total_loss / num_samples, total_dist_err / num_samples


# ================= ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆè®­ç»ƒ | Chunk: {CHUNK_SIZE} | Batch: {BATCH_SIZE} | Device: {DEVICE}")

    if not os.path.exists(H5_PATH):
        print(f"ã€é”™è¯¯ã€‘æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {H5_PATH}")
        return

    # 1. æ‰“å¼€ HDF5 (åªè¯»å–å…ƒæ•°æ®)
    f = h5py.File(H5_PATH, 'r')
    total_samples = len(f['iq'])
    print(f"æ•°æ®é›†æ€»æ ·æœ¬æ•°: {total_samples}")

    # 2. åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›† (90% / 10%)
    all_indices = np.arange(total_samples)
    split_idx = int(0.9 * total_samples)
    train_indices_all = all_indices[:split_idx]
    val_indices_all = all_indices[split_idx:]

    print(f"è®­ç»ƒé›†: {len(train_indices_all)}, éªŒè¯é›†: {len(val_indices_all)}")

    # 3. åˆå§‹åŒ–æ¨¡å‹
    sample_iq = f['iq'][0]
    num_rx = sample_iq.shape[0] // 2

    model = PhysicsGuidedNet(num_rx=num_rx, signal_len=2048).to(DEVICE)

    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)

    # ã€ä¿®å¤ã€‘ç§»é™¤äº† verbose=Trueï¼Œé˜²æ­¢æŠ¥é”™
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6
    )

    criterion_coord = nn.MSELoss()

    # 1. å®šä¹‰ Dice Loss ç±» (è§£å†³å½¢çŠ¶æ¨¡ç³Š)
    class DiceLoss(nn.Module):
        def __init__(self, smooth=1.0):
            super(DiceLoss, self).__init__()
            self.smooth = smooth

        def forward(self, pred_logits, target):
            # å°† Logits è½¬ä¸ºæ¦‚ç‡ (0-1)
            pred_probs = torch.sigmoid(pred_logits)

            # å±•å¹³æ‰€æœ‰ç»´åº¦ï¼Œåªè®¡ç®—é‡å åº¦
            pred_flat = pred_probs.view(-1)
            target_flat = target.view(-1)

            intersection = (pred_flat * target_flat).sum()

            # Dice ç³»æ•° = 2 * äº¤é›† / (å¹¶é›† + å¹³æ»‘é¡¹)
            dice = (2. * intersection + self.smooth) / (pred_flat.sum() + target_flat.sum() + self.smooth)

            return 1 - dice

    # 2. å®šä¹‰åŠ æƒ BCE (è§£å†³æ­£è´Ÿæ ·æœ¬ä¸å¹³è¡¡)
    # å‡è®¾çº¿æ¡åƒç´ å¾ˆå°‘ï¼Œç»™äºˆ 20 å€æƒé‡ï¼Œå¼ºè¿«ç½‘ç»œå…³æ³¨ç™½è‰²çº¿æ¡
    pos_weight = torch.tensor([20.0]).to(DEVICE)

    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    criterion_dice = DiceLoss()

    best_err = float('inf')

    # ================= è®­ç»ƒå¾ªç¯ =================
    try:
        for epoch in range(EPOCHS):
            model.train()
            train_loss_epoch = 0.0

            # è¿›åº¦æ¡
            pbar = tqdm(total=len(train_indices_all), desc=f"Epoch {epoch + 1}/{EPOCHS}")

            # --- Chunk Loading: åˆ†å—è¯»å…¥å†…å­˜ ---
            for chunk_start in range(0, len(train_indices_all), CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, len(train_indices_all))

                # A. ç¡¬ç›˜ -> å†…å­˜ (RAM)
                iq_ram = torch.from_numpy(f['iq'][chunk_start:chunk_end])
                map_ram = torch.from_numpy(f['heatmap'][chunk_start:chunk_end])
                mask_ram = torch.from_numpy(f['mask'][chunk_start:chunk_end])
                coord_ram = torch.from_numpy(f['coord'][chunk_start:chunk_end])

                # B. å†…å­˜ -> DataLoader
                mem_dataset = TensorDataset(iq_ram, map_ram, coord_ram, mask_ram)
                train_loader = DataLoader(mem_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

                # C. GPU è®­ç»ƒ
                for iq, heatmap, true_coord, mask in train_loader:
                    iq, heatmap = iq.to(DEVICE), heatmap.to(DEVICE)
                    mask, true_coord = mask.to(DEVICE), true_coord.to(DEVICE)

                    optimizer.zero_grad()

                    # å‰å‘ä¼ æ’­
                    pred_coord, pred_mask = model(iq, heatmap)

                    # 3D æ ‡ç­¾åˆ‡ç‰‡ä¸º 2D
                    true_coord_xy = true_coord[:, :2]

                    loss_c = criterion_coord(pred_coord, true_coord_xy)
                    # 1. åƒç´ çº§åˆ†ç±» Loss (å¸¦æƒé‡)
                    loss_bce = criterion_bce(pred_mask, mask)

                    # 2. å½¢çŠ¶çº§ Dice Loss
                    loss_dice = criterion_dice(pred_mask, mask)

                    # 3. ç»„åˆ Mask Loss
                    loss_m = loss_bce + loss_dice

                    # åŠ¨æ€æƒé‡è°ƒæ•´ï¼šå‰æœŸä¾§é‡å­¦å½¢çŠ¶(Mask)ï¼ŒåæœŸä¾§é‡ä¿®åæ ‡(Coord)
                    # å¦‚æœ epoch å°äº 20ï¼ŒMask çš„æƒé‡ç»™å¤§ä¸€ç‚¹ (0.5)ï¼Œè®© Mask å…ˆæˆå‹
                    mask_weight = 0.5 if epoch < 20 else 0.1

                    loss = loss_c + mask_weight * loss_m

                    loss.backward()
                    optimizer.step()

                    train_loss_epoch += loss.item() * iq.size(0)

                # æ›´æ–°è¿›åº¦æ¡
                current_lr = optimizer.param_groups[0]['lr']
                pbar.update(chunk_end - chunk_start)
                pbar.set_postfix({'Loss': f"{loss.item():.4f}", 'LR': f"{current_lr:.1e}"})

                # é‡Šæ”¾å†…å­˜
                del iq_ram, map_ram, mask_ram, coord_ram, mem_dataset, train_loader

            pbar.close()

            # --- éªŒè¯é˜¶æ®µ ---
            print("æ­£åœ¨éªŒè¯...")
            avg_val_loss, avg_dist_err = validate(
                model,
                val_indices_all,
                f,
                criterion_coord,
                criterion_bce,
                criterion_dice,
                chunk_size=CHUNK_SIZE,
                batch_size=BATCH_SIZE
            )

            avg_train_loss = train_loss_epoch / len(train_indices_all)
            print(
                f"Epoch {epoch + 1} ç»“æœ: Train Loss={avg_train_loss:.5f}, Val Loss={avg_val_loss:.5f}, å¹³å‡è¯¯å·®={avg_dist_err:.2f}m")

            # --- å­¦ä¹ ç‡è°ƒæ•´ (æ‰‹åŠ¨å®ç° Verbose) ---
            last_lr = optimizer.param_groups[0]['lr']
            scheduler.step(avg_dist_err)
            new_lr = optimizer.param_groups[0]['lr']

            if new_lr != last_lr:
                print(f"ğŸ“‰ å­¦ä¹ ç‡è‡ªåŠ¨è¡°å‡: {last_lr:.1e} -> {new_lr:.1e}")

            # --- ä¿å­˜æ¨¡å‹ ---
            if avg_dist_err < best_err:
                best_err = avg_dist_err
                torch.save(model.state_dict(), "best_model_final.pth")
                print(f">>> å‘ç°æ–°æœ€ä¼˜æ¨¡å‹ï¼è¯¯å·®: {best_err:.2f}mï¼Œå·²ä¿å­˜ã€‚")

    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­ã€‚")
    finally:
        f.close()
        print("HDF5 æ–‡ä»¶å¥æŸ„å·²å…³é—­ã€‚")


if __name__ == '__main__':
    main()