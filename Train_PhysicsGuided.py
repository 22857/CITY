import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import os
import numpy as np

# å¯¼å…¥ä½ å®šä¹‰çš„æ¨¡å—
from PhysicsGuidedNetwork import PhysicsGuidedNet
from PhysicsGuidedDataset import PhysicsGuidedHDF5Dataset

# ================= 1. è·¯å¾„ä¸ç¡¬ä»¶é…ç½® =================
# è¯·ç¡®ä¿æ–‡ä»¶åä¸æœåŠ¡å™¨ä¸Šçš„å®é™…æ–‡ä»¶åä¸€è‡´
H5_PATH = "/root/autodl-tmp/merged_dataset_512_3d_fast_v2.h5"
SAVE_PATH = "best_model_symmetric.pth"
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ================= 2. è¶…å‚æ•°é…ç½® =================
BATCH_SIZE = 64
NUM_WORKERS = 8
LR = 1e-4
EPOCHS = 50
SCENE_SIZE = 5000.0


# ================= 3. å·¥å…·å‡½æ•° =================

def apply_augmentation(iq, heatmap, coord, mask):
    """
    åœ¨ GPU ä¸Šè¿›è¡Œæ•°æ®å¢å¼ºï¼Œä¿æŒ IQ é€šé“ä¸å‡ ä½•ç¿»è½¬çš„ä¸€è‡´æ€§
    """
    # éšæœºæ°´å¹³ç¿»è½¬ (H-Flip)
    if np.random.rand() > 0.5:
        heatmap = torch.flip(heatmap, [3])
        mask = torch.flip(mask, [3])
        coord[:, 0] = 1.0 - coord[:, 0]
        # H-Flip ç´¢å¼•äº¤æ¢: Rx0<->Rx1, Rx3<->Rx2
        idx_perm = torch.tensor([1, 0, 3, 2, 5, 4, 7, 6], device=iq.device)
        iq = iq[:, idx_perm, :]

    # éšæœºå‚ç›´ç¿»è½¬ (V-Flip)
    if np.random.rand() > 0.5:
        heatmap = torch.flip(heatmap, [2])
        mask = torch.flip(mask, [2])
        coord[:, 1] = 1.0 - coord[:, 1]
        # V-Flip ç´¢å¼•äº¤æ¢: Rx0<->Rx3, Rx1<->Rx2
        idx_perm = torch.tensor([3, 2, 1, 0, 7, 6, 5, 4], device=iq.device)
        iq = iq[:, idx_perm, :]

    return iq, heatmap, coord, mask


def get_spatial_weight(target_coord, device):
    """
    æƒé‡æ©ç ï¼šè¾¹ç¼˜åŒºåŸŸæƒé‡ä¸º 0ï¼Œä¸­å¿ƒåŒºåŸŸæƒé‡ä¸º 1
    """
    x = target_coord[:, 0]
    y = target_coord[:, 1]
    MARGIN = 0.1

    # ä¹Ÿå°±æ˜¯ï¼šx,y éƒ½åœ¨ [0.1, 0.9] ä¹‹é—´æ—¶ï¼Œweight=1ï¼Œå¦åˆ™=0
    in_center = (x > MARGIN) & (x < 1.0 - MARGIN) & \
                (y > MARGIN) & (y < 1.0 - MARGIN)

    return in_center.float().unsqueeze(1).to(device)

class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred_logits, target):
        pred_probs = torch.sigmoid(pred_logits)
        intersection = (pred_probs * target).sum()
        dice = (2. * intersection + self.smooth) / (pred_probs.sum() + target.sum() + self.smooth)
        return 1 - dice


def validate(model, loader):
    model.eval()
    total_dist_err = 0.0
    num_samples = 0

    # å®šä¹‰å®‰å…¨åŒºè¾¹ç•Œï¼šå‰”é™¤å››å‘¨å„ 10% (500m) çš„åŒºåŸŸ
    # åªä¿ç•™ x å’Œ y éƒ½åœ¨ [0.1, 0.9] èŒƒå›´å†…çš„æ ·æœ¬
    MARGIN = 0.1

    with torch.no_grad():
        for iq, heatmap, coord, mask in loader:
            iq, heatmap, coord, mask = iq.to(DEVICE), heatmap.to(DEVICE), coord.to(DEVICE), mask.to(DEVICE)

            with torch.cuda.amp.autocast():
                pred_coord, _ = model(iq, heatmap)

            # è®¡ç®—è¯¯å·® (ç±³)
            dist_err = torch.norm(pred_coord - coord[:, :2], dim=1) * SCENE_SIZE

            # --- ä¿®æ”¹åçš„è¿‡æ»¤é€»è¾‘ï¼šçŸ©å½¢è£å‰ª ---
            x, y = coord[:, 0], coord[:, 1]

            # åªæœ‰åœ¨ä¸­å¿ƒçŸ©å½¢åŒºåŸŸå†…çš„æ‰ç®—æ•°
            valid_mask = (x > MARGIN) & (x < 1.0 - MARGIN) & \
                         (y > MARGIN) & (y < 1.0 - MARGIN)

            if valid_mask.sum() > 0:
                total_dist_err += dist_err[valid_mask].sum().item()
                num_samples += valid_mask.sum().item()

    if num_samples == 0: return 9999.0
    return total_dist_err / num_samples


# ================= 4. ä¸»è®­ç»ƒç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨ç»ˆæç‰ˆè®­ç»ƒ (Spatial Weight + Consistency) | è®¾å¤‡: {DEVICE}")

    # 1. åŠ è½½æ•°æ®é›†
    full_dataset = PhysicsGuidedHDF5Dataset(H5_PATH)
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_ds, val_ds = random_split(full_dataset, [train_size, val_size])

    # 2. DataLoader
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, pin_memory=True, prefetch_factor=2)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=4, pin_memory=True)

    # 3. æ¨¡å‹åˆå§‹åŒ–
    model = PhysicsGuidedNet(num_rx=4, signal_len=2048).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)
    scaler = torch.cuda.amp.GradScaler()

    # 4. Loss å®šä¹‰
    # å…³é”®ä¿®æ”¹ï¼šreduction='none' ä»¥ä¾¿æ‰‹åŠ¨åº”ç”¨ç©ºé—´æƒé‡
    criterion_coord = nn.L1Loss(reduction='none')
    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([20.0]).to(DEVICE))
    criterion_dice = DiceLoss()

    best_err = float('inf')

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}")

        for iq, heatmap, coord, mask in pbar:
            iq, heatmap, coord, mask = iq.to(DEVICE), heatmap.to(DEVICE), coord.to(DEVICE), mask.to(DEVICE)

            # 1. åŸºç¡€å¢å¼º
            iq, heatmap, coord, mask = apply_augmentation(iq, heatmap, coord, mask)

            optimizer.zero_grad()

            # --- Pass A: åŸå§‹å‰å‘ä¼ æ’­ ---
            with torch.cuda.amp.autocast():
                pred_coord, pred_mask = model(iq, heatmap)

                # A1. è®¡ç®—åæ ‡ Loss (å¸¦ç©ºé—´åŠ æƒ)
                raw_loss_c = criterion_coord(pred_coord, coord[:, :2])  # [B, 2]
                spatial_w = get_spatial_weight(coord, DEVICE)  # [B, 1]
                loss_c = (raw_loss_c * spatial_w).mean()  # Scalar

                # A2. è®¡ç®— Mask Loss
                loss_m = criterion_bce(pred_mask, mask) + criterion_dice(pred_mask, mask)

            # --- Pass B: ä¸€è‡´æ€§çº¦æŸ (Explicit Consistency) ---
            loss_consistency = torch.tensor(0.0, device=DEVICE)

            # 100% è§¦å‘ä¸€è‡´æ€§æ£€æŸ¥
            if True:
                # B1. æ„é€ ç¿»è½¬æ ·æœ¬ (H-Flip)
                heatmap_flip = torch.flip(heatmap, [3])
                idx_perm = torch.tensor([1, 0, 3, 2, 5, 4, 7, 6], device=DEVICE)
                iq_flip = iq[:, idx_perm, :]

                with torch.cuda.amp.autocast():
                    # B2. é¢„æµ‹
                    pred_coord_flip, _ = model(iq_flip, heatmap_flip)

                # B3. è¿˜åŸåæ ‡: x' = 1 - x
                pred_coord_restored = pred_coord_flip.clone()
                pred_coord_restored[:, 0] = 1.0 - pred_coord_restored[:, 0]

                # B4. è®¡ç®—ä¸€è‡´æ€§ (L1 Loss)
                loss_consistency = torch.nn.functional.l1_loss(pred_coord, pred_coord_restored.detach())

            # --- æ€» Loss ---
            mask_w = 0.5 if epoch < 20 else 0.3
            # Consistency æƒé‡ç»™ 2.0ï¼Œå¼ºè¿«æ¨¡å‹å­¦ä¼šè‡ªæ´½
            total_loss = loss_c + mask_w * loss_m + 0 * loss_consistency

            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += total_loss.item()
            pbar.set_postfix({
                'Loss': f"{total_loss.item():.3f}",
                'Consis': f"{loss_consistency.item():.3f}"
            })

        # éªŒè¯
        val_err = validate(model, val_loader)
        print(f"Epoch {epoch + 1} éªŒè¯å®Œæˆ: å¹³å‡è¯¯å·® = {val_err:.2f}m")

        scheduler.step(val_err)

        if val_err < best_err:
            best_err = val_err
            torch.save(model.state_dict(), SAVE_PATH)
            print(f"ğŸŒŸ å‘ç°æ›´ä¼˜æ¨¡å‹: {best_err:.2f}m")


if __name__ == '__main__':
    main()