import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm
import h5py
import numpy as np
import os
from PhysicsGuidedNetwork import PhysicsGuidedNet

# ================= è·¯å¾„é…ç½® =================
H5_PATH = r"D:\Dataset\SignalDataset\merged_dataset_512_3d_fast.h5"

# ================= è¶…å‚æ•°é…ç½® =================
BATCH_SIZE = 32
CHUNK_SIZE = 2000
LR = 1e-4
EPOCHS = 50
SCENE_SIZE = 5000.0
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


# ================= 1. ç‰©ç†ä¿®æ­£ç‰ˆæ•°æ®å¢å¼º (Core Fix) =================
def apply_augmentation(iq, heatmap, coord, mask):
    """
    å¯¹ Batch æ•°æ®è¿›è¡Œéšæœºæ—‹è½¬å’Œç¿»è½¬ (GPUåŠ é€Ÿ)
    å¿…é¡»åŒæ­¥äº¤æ¢ IQ é€šé“ï¼Œä»¥ä¿æŒç‰©ç†ä¸€è‡´æ€§ï¼

    æ¥æ”¶æœºå¸ƒå±€å‡è®¾ (åŸºäº MakeCsvIQData):
    Rx0:(0,0), Rx1:(5000,0), Rx2:(5000,5000), Rx3:(0,5000)

    IQ æ•°æ®ç»“æ„ (åŸºäº Generate_Multimodal_Data):
    [B, 8, L] -> [Rx0_R, Rx1_R, Rx2_R, Rx3_R, Rx0_I, Rx1_I, Rx2_I, Rx3_I]
    """

    # --- 1. éšæœºæ°´å¹³ç¿»è½¬ (H-Flip) ---
    # å‡ ä½•æ„ä¹‰ï¼šå·¦å³äº’æ¢ -> Rx0<->Rx1, Rx3<->Rx2
    if np.random.rand() > 0.5:
        # A. å›¾ç‰‡ä¸æ ‡ç­¾ç¿»è½¬
        heatmap = torch.flip(heatmap, [3])  # Width is dim 3
        mask = torch.flip(mask, [3])
        coord[:, 0] = 1.0 - coord[:, 0]  # x = 1-x

        # B. IQ é€šé“äº¤æ¢ (å…³é”®ä¿®æ­£!)
        # å®éƒ¨äº¤æ¢: 0<->1, 3<->2
        # è™šéƒ¨äº¤æ¢: 4<->5, 7<->6
        # åŸå§‹ç´¢å¼•: [0, 1, 2, 3, 4, 5, 6, 7]
        # ç›®æ ‡ç´¢å¼•: [1, 0, 3, 2, 5, 4, 7, 6]
        idx_perm = torch.tensor([1, 0, 3, 2, 5, 4, 7, 6], device=iq.device)
        iq = iq[:, idx_perm, :]

    # --- 2. éšæœºå‚ç›´ç¿»è½¬ (V-Flip) ---
    # å‡ ä½•æ„ä¹‰ï¼šä¸Šä¸‹äº’æ¢ -> Rx0<->Rx3, Rx1<->Rx2
    if np.random.rand() > 0.5:
        # A. å›¾ç‰‡ä¸æ ‡ç­¾ç¿»è½¬
        heatmap = torch.flip(heatmap, [2])  # Height is dim 2
        mask = torch.flip(mask, [2])
        coord[:, 1] = 1.0 - coord[:, 1]  # y = 1-y

        # B. IQ é€šé“äº¤æ¢ (å…³é”®ä¿®æ­£!)
        # å®éƒ¨äº¤æ¢: 0<->3, 1<->2
        # è™šéƒ¨äº¤æ¢: 4<->7, 5<->6
        # åŸå§‹ç´¢å¼•: [0, 1, 2, 3, 4, 5, 6, 7]
        # ç›®æ ‡ç´¢å¼•: [3, 2, 1, 0, 7, 6, 5, 4]
        idx_perm = torch.tensor([3, 2, 1, 0, 7, 6, 5, 4], device=iq.device)
        iq = iq[:, idx_perm, :]

    # (å¯é€‰) æ—‹è½¬ 90åº¦ ä¹Ÿå¯ä»¥åŠ äº†ï¼Œå› ä¸º Rx æ˜¯æ­£æ–¹å½¢å¯¹ç§°çš„
    # é€†æ—¶é’ˆ90åº¦: (x,y)->(-y,x)ã€‚Rx0->Rx1->Rx2->Rx3->Rx0
    # å¯¹åº” IQ é€šé“å¾ªç¯ç§»ä½å³å¯ã€‚ä¸ºäº†ç¨³å¦¥ï¼Œå…ˆåªç”¨ Flip è¯•è¯•æ•ˆæœã€‚

    return iq, heatmap, coord, mask


# ================= 2. Dice Loss (ä¿æŒ) =================
class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, pred_logits, target):
        pred_probs = torch.sigmoid(pred_logits)
        pred_flat = pred_probs.view(-1)
        target_flat = target.view(-1)
        intersection = (pred_flat * target_flat).sum()
        dice = (2. * intersection + self.smooth) / (pred_flat.sum() + target_flat.sum() + self.smooth)
        return 1 - dice


# ================= éªŒè¯å‡½æ•° =================
def validate(model, val_indices, h5_file, criterion_coord, criterion_bce, criterion_dice, chunk_size=1000,
             batch_size=32):
    model.eval()
    total_loss = 0.0
    total_dist_err = 0.0
    num_samples = 0

    with torch.no_grad():
        for i in range(0, len(val_indices), chunk_size):
            current_indices = val_indices[i: i + chunk_size]
            current_indices = np.sort(current_indices)

            iq_ram = torch.from_numpy(h5_file['iq'][current_indices]).float()
            heatmap_ram = torch.from_numpy(h5_file['heatmap'][current_indices]).float()
            mask_ram = torch.from_numpy(h5_file['mask'][current_indices]).float()
            coord_ram = torch.from_numpy(h5_file['coord'][current_indices]).float()

            temp_dataset = TensorDataset(iq_ram, heatmap_ram, coord_ram, mask_ram)
            temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

            for iq, heatmap, true_coord, mask in temp_loader:
                iq, heatmap = iq.to(DEVICE), heatmap.to(DEVICE)
                mask, true_coord = mask.to(DEVICE), true_coord.to(DEVICE)

                pred_coord, pred_mask = model(iq, heatmap)

                true_coord_xy = true_coord[:, :2]
                loss_c = criterion_coord(pred_coord, true_coord_xy)

                loss_b = criterion_bce(pred_mask, mask)
                loss_d = criterion_dice(pred_mask, mask)
                loss_total = loss_c + 0.5 * (loss_b + loss_d)

                batch_len = iq.size(0)
                total_loss += loss_total.item() * batch_len

                dist_meter = torch.norm(pred_coord - true_coord_xy, dim=1) * SCENE_SIZE
                total_dist_err += dist_meter.sum().item()

                num_samples += batch_len

            del iq_ram, heatmap_ram, mask_ram, coord_ram, temp_dataset, temp_loader

    return total_loss / num_samples, total_dist_err / num_samples


# ================= ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨ç‰©ç†ä¿®æ­£å¢å¼ºç‰ˆè®­ç»ƒ (Symmetric Rx Augmentation) | Device: {DEVICE}")

    if not os.path.exists(H5_PATH):
        print(f"ã€é”™è¯¯ã€‘æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {H5_PATH}")
        return

    f = h5py.File(H5_PATH, 'r')
    total_samples = len(f['iq'])

    # åˆ’åˆ†æ•°æ®é›†
    all_indices = np.arange(total_samples)
    split_idx = int(0.9 * total_samples)
    train_indices_all = all_indices[:split_idx]
    val_indices_all = all_indices[split_idx:]

    # åˆå§‹åŒ–æ¨¡å‹
    sample_iq = f['iq'][0]
    num_rx = sample_iq.shape[0] // 2
    model = PhysicsGuidedNet(num_rx=num_rx, signal_len=2048).to(DEVICE)

    # ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨
    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6
    )

    # Loss å®šä¹‰
    criterion_coord = nn.L1Loss()
    pos_weight = torch.tensor([20.0]).to(DEVICE)
    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    criterion_dice = DiceLoss()

    best_err = float('inf')

    try:
        for epoch in range(EPOCHS):
            model.train()
            train_loss_epoch = 0.0
            pbar = tqdm(total=len(train_indices_all), desc=f"Epoch {epoch + 1}/{EPOCHS}")

            for chunk_start in range(0, len(train_indices_all), CHUNK_SIZE):
                chunk_end = min(chunk_start + CHUNK_SIZE, len(train_indices_all))

                # Load to RAM
                iq_ram = torch.from_numpy(f['iq'][chunk_start:chunk_end])
                map_ram = torch.from_numpy(f['heatmap'][chunk_start:chunk_end])
                mask_ram = torch.from_numpy(f['mask'][chunk_start:chunk_end])
                coord_ram = torch.from_numpy(f['coord'][chunk_start:chunk_end])

                mem_dataset = TensorDataset(iq_ram, map_ram, coord_ram, mask_ram)
                train_loader = DataLoader(mem_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

                for iq, heatmap, true_coord, mask in train_loader:
                    iq, heatmap = iq.to(DEVICE), heatmap.to(DEVICE)
                    mask, true_coord = mask.to(DEVICE), true_coord.to(DEVICE)
                    true_coord_xy = true_coord[:, :2]

                    optimizer.zero_grad()

                    # ================= ä¿®æ”¹ 2: ä¸€è‡´æ€§è®­ç»ƒç­–ç•¥ =================

                    # --- Pass 1: åŸå§‹æ•°æ®å‰å‘ä¼ æ’­ ---
                    pred_coord, pred_mask = model(iq, heatmap)

                    # åŸºç¡€ Loss
                    loss_c = criterion_coord(pred_coord, true_coord_xy)
                    loss_mask = criterion_bce(pred_mask, mask) + criterion_dice(pred_mask, mask)

                    # --- Pass 2: æ„å»ºå¢å¼ºæ ·æœ¬ (ä¸è®¡ç®—æ¢¯åº¦ï¼Œåªç”¨äºç”Ÿæˆä¸€è‡´æ€§ç›®æ ‡? ä¸ï¼Œè¿™é‡Œè¦åŒå‘çº¦æŸ) ---
                    # å¼ºåˆ¶åœ¨è®­ç»ƒå¾ªç¯é‡Œåšä¸€æ¬¡ç¿»è½¬
                    iq_aug, map_aug, coord_aug, _ = apply_augmentation(iq.clone(), heatmap.clone(), true_coord.clone(),
                                                                       mask.clone())

                    # å¯¹å¢å¼ºåçš„æ•°æ®é¢„æµ‹
                    pred_coord_aug, _ = model(iq_aug, map_aug)

                    # è®¡ç®—ä¸€è‡´æ€§ Loss: || Pred_Aug - GT_Aug || (è¿™å…¶å®å°±æ˜¯æ•°æ®å¢å¼ºçš„æ ‡å‡†åšæ³•)
                    # ä½†ä¸ºäº†æ›´å¼ºçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥åŠ ä¸€ä¸ªé¢å¤–çš„çº¦æŸï¼š
                    # Loss_Consistency = || Pred_Aug - Transform(Pred_Original) ||
                    # è¿™é‡Œä¸ºäº†ç®€åŒ–è®¡ç®—ä¸”èŠ‚çœæ˜¾å­˜ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨â€œæ··åˆæ•°æ®å¢å¼ºâ€ç­–ç•¥ï¼š
                    # å³ï¼šå¹¶ä¸æ˜¾å¼è®¡ç®— Consistency Lossï¼Œè€Œæ˜¯ä¾èµ– apply_augmentation
                    # é…åˆ L1 Loss çš„å¼ºå¤§æ¢¯åº¦æ¥éšå¼è¾¾æˆã€‚

                    # ä¿®æ­£ï¼šæ—¢ç„¶æˆ‘ä»¬è¦è¿½æ±‚æè‡´ï¼Œç›´æ¥æŠŠ augment å˜æˆå¿…é€‰é¡¹ï¼Œæˆ–è€…åšä¸¤æ¬¡ forward
                    # è€ƒè™‘åˆ°æ˜¾å­˜ï¼Œæˆ‘ä»¬é‡‡ç”¨ 50% æ¦‚ç‡åšä¸€è‡´æ€§æ­£åˆ™åŒ–

                    loss_consistency = 0.0
                    if np.random.rand() > 0.5:
                        # æ„é€ ç¿»è½¬æ ·æœ¬ (ä»¥æ°´å¹³ç¿»è½¬ä¸ºä¾‹)
                        # ç¿»è½¬è¾“å…¥
                        map_flip = torch.flip(heatmap, [3])
                        # äº¤æ¢ IQ (H-Flip: 0<->1, 2<->3...)
                        idx_perm = torch.tensor([1, 0, 3, 2, 5, 4, 7, 6], device=DEVICE)
                        iq_flip = iq[:, idx_perm, :]

                        # é¢„æµ‹ç¿»è½¬åçš„åæ ‡
                        pred_coord_flip, _ = model(iq_flip, map_flip)

                        # å°†ç¿»è½¬åçš„åæ ‡è¿˜åŸ: x' = 1 - x
                        pred_coord_restored = pred_coord_flip.clone()
                        pred_coord_restored[:, 0] = 1.0 - pred_coord_restored[:, 0]

                        # ä¸€è‡´æ€§ Loss: åŸå§‹é¢„æµ‹ vs è¿˜åŸåçš„ç¿»è½¬é¢„æµ‹
                        # è¿™å¼ºè¿«ç½‘ç»œæ»¡è¶³ç‰©ç†å¯¹ç§°æ€§
                        loss_consistency = criterion_coord(pred_coord, pred_coord_restored.detach()) * 2.0
                        # æ³¨æ„ï¼šè¿™é‡Œç”¨äº† detach()ï¼Œé€šå¸¸ä½œä¸ºæ­£åˆ™é¡¹ï¼Œæˆ–è€…åŒå‘éƒ½ä¼ æ¢¯åº¦ä¹Ÿå¯ä»¥

                    # ========================================================

                    # åŠ¨æ€æƒé‡
                    mask_w = 0.5 if epoch < 20 else 0.1
                    # æ€» Loss = åæ ‡(L1) + Mask + ä¸€è‡´æ€§çº¦æŸ
                    loss = loss_c + mask_w * loss_mask + 0.1 * loss_consistency

                    loss.backward()
                    optimizer.step()

                    train_loss_epoch += loss.item() * iq.size(0)

                pbar.update(chunk_end - chunk_start)
                pbar.set_postfix({'Loss': f"{loss.item():.4f}"})
                del iq_ram, map_ram, mask_ram, coord_ram, mem_dataset, train_loader

            pbar.close()

            print("æ­£åœ¨éªŒè¯...")
            avg_val_loss, avg_dist_err = validate(
                model, val_indices_all, f,
                criterion_coord, criterion_bce, criterion_dice,
                chunk_size=CHUNK_SIZE, batch_size=BATCH_SIZE
            )

            avg_train_loss = train_loss_epoch / len(train_indices_all)
            print(
                f"Epoch {epoch + 1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Err={avg_dist_err:.2f}m")

            scheduler.step(avg_dist_err)

            if avg_dist_err < best_err:
                best_err = avg_dist_err
                torch.save(model.state_dict(), "best_model_symmetric.pth")
                print(f">>> æ–°æœ€ä¼˜æ¨¡å‹ (Symmetric): {best_err:.2f}m")

    except KeyboardInterrupt:
        print("\nè®­ç»ƒä¸­æ–­ã€‚")
    finally:
        f.close()


if __name__ == '__main__':
    main()